\documentclass[11pt]{article}
\input{../notes-preamble}
\newcommand{\patternSpace}{\ensuremath{\mathcal{D}}}
\newcommand{\variablePatternSpace}{\ensuremath{\mathcal{D}_v}}


\begin{document}
{\bf Notes for 848 lecture 6: A ML basis for compatibility and parsimony}
\section*{Notation}
\begin{eqnarray}
	\theta & \in & \Theta 
\end{eqnarray}
\begin{compactitem}
	\item $\Theta$ is the space of all possible trees (and model parameters)
	\item $\theta$ is a point in the parameter space = a particular tree and a set of values for all of the 			\item $a \in b$ means that $a$ is ``in'' $b$.
\end{compactitem}
\begin{eqnarray}
	X & \in & \mathcal{X} 
\end{eqnarray}
\begin{compactitem}
	\item $\mathcal{X}$ is the space of all possible character matrices
	\item $X$ represents our specific data matrix.
\end{compactitem}
Here is a slight abuse of terminology:
\begin{eqnarray}
	X & \sim & \Pr(X=x|\theta)
\end{eqnarray}
\begin{compactitem}
	\item $x$ is a specific ``value'' that a character matrix could assume.  For a character matrix
	the value is the exact set of data patterns found.
	\item $\Pr(X=x|\theta)$ is the probability that our character matrix will have an exact
	composition described by $x$ if $\theta$ is the true value of the parameter.
	\item $a \sim b$ means that $a$ is a random variable drawn from the distribution described by the distribution $b$.  
	We view our data set as drawn from a distribution described by a set of probability statements about what type of matrices could be generated by the course of evolution\footnote{This is a bit of an abuse of terminology because we should say that $X$ is drawn from a distribution, rather than from a probability statement.  
	What we mean, more technically, is that we assume that $X$ is a single draw from a multinomial distribution with the probabilities of the different categories being defined by statements like $\Pr(X=x|\theta)$ where $x$ represents the category of the multinomial.  To formally express this we would have to introduce an indexing scheme for the multinomial. So we'll just be informal.}
\end{compactitem}
\begin{eqnarray}
	\mathcal{L}(\theta) & = & \Pr(X=x|\theta)\\
	\mathcal{L}(\theta) & = & \Pr(X|\theta)
\end{eqnarray}
We will refer to $\Pr(X=x|\theta)$ as the {\bf likelihood} of the model described by $\theta$. 
It is conditional on the fact that we have observed a single data set $X$, that has the characters described
by the point $x$ in the space of all possible datasets.
\begin{compactitem}
	\item $\mathcal{L}(\theta)$ is the notation for the likelihood of ``model'' $\theta$
	\item $\Pr(X|\theta)$ is a simplified notation for $\Pr(X=x|\theta)$
\end{compactitem}

The maximum likelihood estimate of the tree (and model parameters) is $\hat{\theta}$:
\begin{eqnarray}
	\hat{\theta} & = & \arg\max \mathcal{L}(\theta)
\end{eqnarray}
which simply means the value of point in parameter space $\Theta$ for which $ \mathcal{L}(\cdot)$ achieves it highest value. (if there are ties, then there will be multiple (or even an infinite set) of maximum likelihood estimates.


\section*{A homoplasy-free model}
We can build a model of perfect data.  
For simplicity, let's assume that we have binary characters.
First, we assume that homoplasy is impossible then, there can only be 0 changes or 1 change across the whole tree.
A simple assumption (but biologically unrealistic approach) is to say that, when a change occurs it is equally likely to occur on any one of the branches of the unrooted tree.
Finally we'll say that we only considerable variable characters and we polarize the state codes by assigning 0
to the character state displayed in the outgroup (taxon A).
Let's consider the case of the fully-resolved trees for the ingroup B, C, and D.


\subsection*{How many possible character patterns are there?}
We refer to a vector of characters states for each taxon as a pattern (or ``character pattern'' or ``data pattern'').

Imagine that we score human, dog, and cat, and frog for two characters: presence/absence of placenta and presence/absence of hair.
We have two characters, but they both can be expressed as the vector of state codes: 1110  (if we order the taxa such that frog is last).  
If we do code the characters in this way then each characters is a different instance of the same pattern.

We can use $\patternSpace$ to represent the set of all possible patterns (or the ``space of all possible patterns'', if you prefer) and $\variablePatternSpace$ to refer to the set of all variable patterns.
There are $N$ taxa.
Each of the ingroup taxa  (there are $N-1$ ingroup taxa) can display any of the $k$ states, so the number of possible patterns is:
\begin{eqnarray}
	\#\mbox{possible patterns} & = & k^{(N-1)},
\end{eqnarray}
but this includes the all 0 pattern which we said that we would exclude (when we said that we'll just look at variable characters).
\begin{eqnarray}\label{numVarPatterns}
	|\variablePatternSpace| = \#\mbox{possible variable patterns}& = & k^{(N-1)} - 1.
\end{eqnarray}
Here $N=4$ and $k=2$, so there are 7 patterns.
The $|x|$ notation, when applied to a set means the size of the set.

\subsection*{How many possible character matrices are there?} 
That depends on how many characters we sample.
If we sample $M$ characters, then there are simply the number of 
possible character matrices.
\begin{eqnarray}
	\#\mbox{possible matrices of variable patterns} & = & [k^{(N-1)} - 1]^M
\end{eqnarray}
So there are 13,841,287,201 different matrices with 12 characters and 3 ingroup taxa when
we only allow variable characters.

\section*{The likelihood of a character matrix}
If we assume that different characters are independent from each other then 
the probability of the entire matrix is simply the product
of the probabilities of the $M$ different ``events,'' where each event corresponds to
a different characters in the matrix.
Let $X_i$ refer to character number $i$ of the character matrix $X$:
\begin{eqnarray}\label{indepChar}
	\mathcal{L}(\theta) = \Pr(X|\theta) = \prod_{i=1}^{M}\Pr(X_i|\theta)
\end{eqnarray}
This dramatically simplifies our life because we can break of the calculation into $M$ manageable ones.
If two different characters display the same pattern, then they will have the same ``character-likelihood''

\subsection*{Pattern counts}
Because of our assumption of independence, the full likelihood is simply
a product over all characters (as shown in equation \ref{indepChar}, above).
Multiplication is commutative (which means that we can rearrange the term without affecting the result, $ab=ba$).
This means that we can imagine rearranging the order of characters in our matrix without
changing the likelihood -- this is reassuring because we don't have any ``correct'' order
to list our characters in.

Thus, we can also calculate the likelihood by calculating the likelihood from the counts of each type of pattern:
\begin{eqnarray}\label{prodOverPatterns}
	c_i & = & \sum_{j=1}^{M}I(x_j = d_i) \\
	\mathcal{L}(\theta) & = & \Pr(X|\theta) =  \prod_{i=1}^{|\variablePatternSpace|}\Pr(d_i|\theta)^{c_i} \\
\end{eqnarray}
Here $c_i$ is the number of characters in the matrix that display pattern $d_i$.  
There is not widely-used notion for the count of someting, so we show this mathematically
by making it a sum across all characters (that is the $\sum_{j=1}^{M}$ notation) of and indicator function.
An indicator function is a common notational convenience that means ``a value of 1 if the condition is true, and 0 if the condition is false'' So:
\begin{eqnarray}
	I(x_j = d_i) = \left\{ \begin{array}{cc}
1  &  \mbox{if character } x_j \mbox{ displays pattern } d_i   \\
0 & \mbox{otherwise}   
\end{array} \right.
\end{eqnarray}

\section*{The likelihood of a character}
Under our 2-state  model of homoplasy-free variable patterns, each unrooted tree can generate 5 data patterns and each patterns are generated with a probability of $0.2 = \frac{1}{5}$. The patterns correspond to what you would get if you label the leaf A with state 0, and then consider putting one transition on tree.
There are 5 edges in the unrooted trees, such as the one shown in Figure \ref{ablabeledTree}
Table \ref{perfectPatternLikes} shows the likelihood for the 7 types of data pattern on each of the three trees.
Note that the red rows correspond to the data patterns that are informative in a Hennigian analysis.

Our simple model can be seen as a justification of the Hennigian approach in that it behaves
in an identical fashion:
\begin{compactitem}
	\item If the matrix has at least one 0011 pattern (C and D with state 1 and A and B with state 0), then the  B+C tree and the B+D tree will have a likelihood of 0.
	\item If the matrix has at least one 0101 pattern, then the  B+C tree and the C+D tree will have a likelihood of 0.
	\item If the matrix has at least one 0110 pattern, then the  B+D tree and the C+D tree will have a likelihood of 0.
	\item patterns that correspond to autapomorphies do not provide any phylogenetic information because they contribute the same character likelihood (0.2) to every tree.  
\end{compactitem}
So the presence of only one type of synapomorphy leads to the corresponding tree to be the maximum likelihood estimate of the tree under our model.


\begin{table}[htdp]
\caption{Pattern likelihoods for variable perfect data model}\label{perfectPatternLikes}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
& $T_1$ & $T_2$ & $T_3$ \\
& (C+D) & (B+D) & (B+C) \\
\hline
0001 & 0.2 & 0.2  & 0.2  \\
0010 & 0.2 & 0.2  & 0.2  \\
{\color{red} 0011}&{\color{red}  0.2} &{\color{red}  0.0 } &{\color{red}  0.0}  \\
0100 & 0.2 & 0.2  & 0.2  \\
{\color{red} 0101} &{\color{red}  0.0 } &{\color{red}  0.2  } &{\color{red}  0.0  }\\
{\color{red} 0110} &{\color{red}  0.0 } & {\color{red}  0.0  } &{\color{red}  0.2}  \\
0111 & 0.2 & 0.2  & 0.2  \\
\hline
\end{tabular}
\end{center}
\label{default}
\end{table}%

\begin{figure}[htpd]
\begin{center}
\caption{The unrooted tree $AB|CD$ with edges labelled.  Internal nodes are labelled in red.}
\label{ablabeledTree}
\begin{picture}(0,0)(20,20)
	\thicklines
	\put(-105,3){A}
	\put(-105,-99){B}
	\put(70,3){D}
	\put(70,-99){C}
	\put(-75,-17){$1$}
	\put(-70,-72){$2$}
	\put(40,-17){$3$}
	\put(33,-74){$4$}
	\put(-15,-40){$5$}
	\put(-95,0){\line(1,-1){45}}
	\put(-95,-90){\line(1,1){45}}
	\put(-50,-43){\color{red}E}
	\put(15,-43){\color{red}F}
	\put(-50,-45){\line(1,0){70}}
	\put(20,-45){\line(1,1){45}}
	\put(20,-45){\line(1,-1){45}}
\end{picture}
\end{center}
\vskip 4.1cm
\end{figure}


\section*{A model that can explain character conflict}
The chief difficulty with the model that we just formulated is that it predicts {\em no}
character conflict.
So if a data matrix displays any character conflict, then all trees will have a
likelihood of 0 -- we will not be able to infer a tree and it will be obvious that our model is wrong.
Given that almost all data sets display character conflict, the model is clearly
inappropriate for real data.

Why do we get character conflict? What types of characters are likely to end up in our 
matrix? We must be more specific in order to construct a full probability that will allow
us to infer trees using ML

One possibility is that some characters that we 
have scored in our matrix are simply random noise mistaken as biologically significant.
Imagine a  trait that is plastic in all species such that the variability that we see
phenotypically is the result of environmental variation, and that this environmental 
variability is patchy and essentially random with respect to the phylogeny.
This is not a terribly likely situation, I'll admit.

If this were the case, then we can think of our matrix being composed of two classes
of characters: the first being the informative characters that we are trying to collect, 
and a second class of ``noise'' characters that contaminate the matrix.

We have described a type of model is normally called a ``mixture model.''
There are two classes of random processes acting and our data is a mixture of columns
drawn from each of the process.
To make things clear, lets use $\Pr(d_i|T_1,p)$ to represent the probability that the
our homoplasy-free model (from above) would produce the pattern $d_i$
under tree 1; the $p$ will remind us that this is the perfect data model.
We'll use $\Pr(d_i|T_1,n)$ to indicate the probability of the same pattern under the 
``noise'' model (hence the $n$).

When we consider a character we do not know whether it is coming from the perfect model
class or the noise model.
We can deal with this by summing over each category while weighting by the probability
that the site belongs to that category.

Basic probability states that:
\begin{eqnarray}
	\Pr(A) = \Pr(B)\Pr(A|B) + (1-\Pr(B))\Pr(A|-B)
\end{eqnarray}
where $-{B}$ means ``not B" (borrowing the notation from the complement meaning ``anything other than $B$'' in the set context).
For our mixture model, we can construct:
\begin{eqnarray}
	\Pr(d_i|T_j) = \Pr(\mbox{perfect model})\Pr(d_i|T_j,p) + \Pr(\mbox{noise})\Pr(d_i|T_j,n)
\end{eqnarray}

We do not know the probability that a character that we score will be from the perfect model category.
So we will introduce a parameter into our model that represents the proportion of time that we accidentally include noise in our matrix.
We do not know the correct value for this parameter.
Let's call this parameter $\epsilon$ (the Greek letter epsilon), it will represent the
{\em a priori} probability that a character is random, ``white'' noise.
You can also think of $\epsilon$ as the proportion of characters that are noise if we
had an infinite number of characters in our matrix -- or think of it as the expectation
of the proportion of noise.
\begin{eqnarray}
	\Pr(d_i|T_j) = (1- \epsilon)\Pr(d_i|T_j,p) + \epsilon\Pr(d_i|T_j,n)
\end{eqnarray}
We already discussed how to calculate $\Pr(d_i|T_j,p)$ (these probabilities were shown in table \ref{perfectPatternLikes}).
How do we calculate $\Pr(d_i|T_j,n)$, the probability of a data pattern under the assumption that
the character is pure noise?
If we assume that each of the $k$ characters states are equally probable, then the probability of a pattern
is simply $\left(\frac{1}{k}\right)^N$.
Of course, we are conditioning our data on being variable and having state 0 in the outgroup.
Recall that early we calculated the number of such patterns (equation \ref{numVarPatterns}).
If they occur at the same frequency, then we can see that:
\begin{eqnarray}
	\Pr(d_i|T_j,n) = \frac{1}{|\variablePatternSpace|} = \frac{1}{7}
\end{eqnarray}
Because the ``noise'' model reveals no phylogenetic information:
\begin{eqnarray}
	\Pr(d_i|T_1,n) = \Pr(d_i|T_2,n) = \Pr(d_i|T_3,n) = \frac{1}{7}
\end{eqnarray}

We can now see that if a pattern, $d$, is compatible with a tree, $T$, then the likelihood will be:
\begin{eqnarray}
	\Pr(d|T) = \frac{(1-\epsilon)}{5} + \frac{\epsilon}{7}
\end{eqnarray}
and if a pattern is incompatible with a tree, then the probability that we would see that pattern
under the perfect model is 0 so we have:
\begin{eqnarray}
	\Pr(d|T) = \frac{\epsilon}{7}
\end{eqnarray}

To be more formal we can denote the set of patterns that are compatible with a tree as $D(T)$. 
Thus,
\begin{eqnarray}\label{specialPerfectNoise}
	\Pr(d_i|T_j) = \left\{ \begin{array}{cc}
\frac{(1-\epsilon)}{5} + \frac{\epsilon}{7} &  \mbox{if } d_i \in D(T_j)   \\ 
	\frac{\epsilon}{7} &  \mbox{if } d_i \notin D(T_j)   \\
\end{array} \right.
\end{eqnarray}

Note that:
\[\frac{(1-\epsilon)}{5} + \frac{\epsilon}{7} = (7-2\epsilon)/35 \]

Table \ref{contaminatedModelPatternLikes} shows the pattern likelihood
under our ``perfect+noise'' mixture model.
We can see that if $\epsilon=0$, then the model collapses to the perfect-data model.
If $\epsilon=1$, then all characters are from the noise model; every pattern
has the probability of $\frac{1}{7}$ regardless of tree, and we can no longer infer trees (from any dataset).

\begin{table}[htdp]
\caption{Pattern likelihoods for under the perfect+noise mixture model}\label{contaminatedModelPatternLikes}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
& $T_1$ & $T_2$ & $T_3$ \\
& (C+D) & (B+D) & (B+C) \\
\hline
0001 & $(7-2\epsilon)/35$ & $(7-2\epsilon)/35$  & $(7-2\epsilon)/35$  \\
0010 & $(7-2\epsilon)/35$ & $(7-2\epsilon)/35$  & $(7-2\epsilon)/35$  \\
{\color{red} 0011}&{\color{red}  $(7-2\epsilon)/35$} &{\color{red}  $\epsilon/7$ } &{\color{red}  $\epsilon/7$}  \\
0100 & $(7-2\epsilon)/35$ & $(7-2\epsilon)/35$  & $(7-2\epsilon)/35$  \\
{\color{red} 0101} &{\color{red}  $\epsilon/7$ } &{\color{red}  $(7-2\epsilon)/35$  } &{\color{red}  $\epsilon/7$  }\\
{\color{red} 0110} &{\color{red}  $\epsilon/7$ } & {\color{red}  $\epsilon/7$  } &{\color{red}  $(7-2\epsilon)/35$}  \\
0111 & $(7-2\epsilon)/35$ & $(7-2\epsilon)/35$  & $(7-2\epsilon)/35$  \\
\hline
\end{tabular}
\end{center}
\label{default}
\end{table}%

\subsection*{General formulae for the perfect+noise mixture model}
Above (equation \ref{specialPerfectNoise}) we presented the probability of a pattern on a four taxon 
tree with variable, 2-state characters scored.
The more general formulae for probabilities of the patterns depend on the number of states and the
number of branches in the tree.
In our ``perfect data'' (homoplasy-free) model a state changes occur across each branch with
equal frequency, so the probability of each resulting pattern is just 1 over the number of edges in the
tree.
Fortunately there is a simple formula for the number of edges in a fully-resolved tree (or the size of the
set of edges, $E$):
\begin{eqnarray}
	|E| = \mbox{\# edges} = 2N-3
\end{eqnarray}
(I always remember this formula by remembering that every time we add a taxon we add to edges to the tree - one
new terminal edge, and one edge in the previous tree is split into two edges in the new tree.
This implies that the coefficient for $N$ will be 2.  
Then I remember that an unrooted two-taxon tree has one edges, so we must subtract three.)

The most general form of the perfect+noise mixture model would work for any number of states, but that
gets complicated (because we have to consider multiple transition for a character across the tree
even though there is no homoplasy).
We will simply note that for two states we have:
\begin{eqnarray}
	\Pr(d_i|T_j) = \left\{ \begin{array}{cc}
\frac{(1-\epsilon)}{2N-3} + \frac{\epsilon}{2^{(N-1)} - 1} &  \mbox{if } d_i \in D(T_j)   \\ 
	\frac{\epsilon}{2^{(N-1)} - 1} &  \mbox{if } d_i \notin D(T_j)   \\
\end{array} \right.
\end{eqnarray}

\subsection*{The ML tree}
\subsubsection*{ML under the perfect+noise model is the max compatibility tree}
The first problem that we face if we want to calculate the likelihood is the fact that we do
not know the correct value for $\epsilon$.
One of the most popular (and reliable) ways to deal with this issue is too estimate
both the tree and $\epsilon$ using ML.
Interestingly, in this case we will find that we do not even need to estimate the correct
value of $\epsilon$ to infer the tree.

We can see that this is the case by ``reparameterizing'' the model.
Let:
\begin{eqnarray}
	\phi & = & \frac{(7-2\epsilon)/35}{\epsilon/7} \\
		& = & \frac{(7-2\epsilon)/5}{\epsilon}\\
		& = & \frac{(7-2\epsilon)}{5\epsilon} \\
		& = & \frac{1.4}{\epsilon} - 0.4
\end{eqnarray}
Here $\phi$ is simple function of $\epsilon$, and $\phi > 1$ for range $0<\epsilon < 1$.

When we express the likelihood for the patterns in the new parameterization we see that every
probability is either $\epsilon/7$ (for all patterns that conflict with the tree), or $\phi\epsilon/7$
(for all patterns that are compatible with the tree.
This means that we can rephrase equation \ref{prodOverPatterns} as:
\begin{eqnarray}
	\Pr(X|T_j)  & = & \prod_{i=1}^{|\variablePatternSpace|}\Pr(d_i|\theta)^{c_i} \\
				& = & \left(\prod_{d_i\in D(T_j)}\Pr(d_i|\theta)^{c_i} \right) \left(\prod_{d_i\notin D(T_j)}\Pr(d_i|\theta)^{c_i} \right)\\
				& = & \left(\prod_{d_i\in D(T_j)}[\phi\epsilon/7] \right) \left(\prod_{d_i\notin D(T_j)}[\epsilon/7] \right)\\
				& = & \phi^{C(T_j)}\left(\frac{\epsilon}{7}\right)^{M} \label{simpleCompatLike}
\end{eqnarray}
where $C(T_j)$ is the sum of the counts of the characters that are compatible with tree $T_j$:
\begin{eqnarray}
	C(T_j)  & = & \sum_{d_i\in D(T_j)}c_i
\end{eqnarray}

Note that our likelihood in equation \ref{simpleCompatLike} has a two factors:
the first depends on $\epsilon$ and the count of the number of characters in the data matrix that are
compatible with the tree that we are scoring;
the second factor does not depend on the data set or the tree we are scoring.
Because the second factor appears in the likelihood equation for every tree, we can simply ignore it
when comparing trees.
The tree with the largest $\phi^{C(T_j)}$ will have the highest likelihood.
Because $\phi$ is always greater than one (for the restrictions that we have put on it),
the tree with for which $C(T_j)$ is greatest will have the highest likelihood.

If we want to know exactly how much higher the likelihood would be, we would have to estimate a value
for $\epsilon$ (which would then give us a value for $\phi$).
But if we just want to find the ML tree, we don't need to even calculate the likelihood!

The preferred tree is the tree preferred by the maximum compatibility criterion for phylogenies.
This result holds for trees with any number of taxa, and any number of states, though the formulae are
more complex.
See chapter 8 of Felsenstein's book for a nice description of the compatibility approach to
tree inference.


\subsubsection*{Finding the ML tree}
We could find the tree with the highest likelihood by examining every tree. 
This is not practical for even moderate (10-20) numbers of taxa because there are far too many trees
to consider (we'll discuss this later).

In the case of finding the maximum compatibility tree, though, we can take a different approach.
We can construct a compatibility graph.
Consider a graph with nodes representing every pattern that occurs in the character matrix.
Each node has a weight that is equal to the number of times that pattern was seen in the data matrix.
If two patterns are compatible with each other, then we introduce an edge in our graph between the
corresponding nodes.

We can also view the nodes in the graph as representing splits in the tree (because of the one-to-one mapping
between splits and binary characters that we talked about last time).

\noindent{\bf The pairwise compatibility theorem} says:\\
We have a collection of splits,  the entire collection of splits can be fit onto one tree if and only if
every possible pair of splits in the set are pairwise compatible.


So if we look in the compatibility graph and find a subgraph for which there is an edge 
between every pair of nodes, then all of the splits represented by that subgraph can be 
put onto the same tree.
Such subgraph is called a clique.
If we consider the weight of a clique to be the sum of the counts associated with the
nodes in the clique, then finding the clique with the highest weight corresponds 
to finding the largest set of compatible characters that can be mapped onto a single 
tree.
This is tree will be the maximum compatibility tree.
See \citet{Felsenstein2004} (Chapter 8), \citep{Buneman1971} and   \citep{EstabrookM1980} for details and 
proofs.

As discussed by Felsenstein, the maximal compatibility problem is NP-hard, but for
phylogenetic problems, heuristic approaches often work well (the compatibility graphs
in many phylogenetic problems are simple enough to find the maximum compatibility 
tree quickly).

\begin{table}[htdp]
\begin{center}
\caption{Data matrix with some character conflict}\label{coloredHomoplasy}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline 
 & \multicolumn{12}{c|}{Character \#} \\ 
Taxon &\color{blue} 1 & \color{blue} 2 & \color{blue} 3 & \color{blue} 4 & \color{blue} 5 & \color{green} 6 & \color{green} 7 & \color{green} 8 & \color{green} 9 & \color{red} 10 & \color{red} 11 &  \color{red} 12   \\ 
\hline 
A & \color{blue} 0 & \color{blue} 0 & \color{blue} 0 & \color{blue} 0 & \color{blue} 0 & \color{green} 0 & \color{green} 0 & \color{green} 0 & \color{green} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0  \\
B & \color{blue} 1 & \color{blue} 0 & \color{blue} 0 & \color{blue} 0 & \color{blue} 0 & \color{green} 1 & \color{green} 1 & \color{green} 1 & \color{green} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1 \\
C &    \color{blue} 0 & \color{blue} 1 & \color{blue} 1 & \color{blue} 1 & \color{blue} 0 & \color{green} 1 & \color{green} 1 & \color{green} 1 & \color{green} 1 & \color{red} 1 & \color{red} 1 & \color{red} 0\\
D &    \color{blue} 0 & \color{blue} 0 & \color{blue} 0 & \color{blue} 0 & \color{blue} 1 & \color{green} 1 & \color{green} 1 & \color{green} 1 & \color{green} 1 & \color{red} 0 & \color{red} 0 & \color{red} 1\\
\hline 
\end{tabular}
\end{center}
\end{table}

\begin{table}[htdp]
\begin{center}
\caption{Patterns and counts for the data shown in table \ref{coloredHomoplasy}}\label{compressed}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & \multicolumn{6}{c|}{Character } \\ 
Taxon & \color{blue} 1 & \color{blue} 2-4 &  \color{blue} 5 & \color{green} 6-9 & \color{red} 10-11 & \color{red} 12 \\
\hline 
A & \color{blue} 0 & \color{blue} 0 &  \color{blue} 0 & \color{green} 0 & \color{red} 0 & \color{red} 0 \\
B & \color{blue} 1 & \color{blue} 0 &  \color{blue} 0 & \color{green} 1 & \color{red} 1 & \color{red} 1 \\
C & \color{blue} 0 & \color{blue} 1 &  \color{blue} 0 & \color{green} 1 & \color{red} 1 & \color{red} 0 \\
D & \color{blue} 0 & \color{blue} 0 &  \color{blue} 1 & \color{green} 1 & \color{red} 0 & \color{red} 1 \\
\hline
count & 1 & 3 & 1 & 4 & 2 & 1 \\
\hline 
\end{tabular}
\end{center}
\end{table}

\newpage
\begin{figure}[htbp]
\begin{center}
\caption{Compatibility graph for the patterns in table \ref{compressed}. 
Character numbers are inside node circles.
Pattern weights are shown next to the nodes.}
\label{default}
\begin{picture}(30,30)(20,-20)
	\put(-95,-153){\makebox(0,0)[l]{\includegraphics[scale=0.5]{../images/Compatbility.pdf}}}
\end{picture}
\end{center}
\end{figure}
\newpage

\begin{figure}[htbp]
\begin{center}
\caption{Compatibility graph for the patterns in table \ref{compressed}, with maximal weighted clique shown in red (total weight of 11).}
\label{default}
\begin{picture}(30,30)(20,-20)
	\put(-95,-153){\makebox(0,0)[l]{\includegraphics[scale=0.5]{../images/MaxCompatbility.pdf}}}
\end{picture}
\end{center}
\end{figure}
\vskip 7cm

As \citet{Felsenstein2004} discusses, the pairwise compatibility theorem does not apply to characters with
more than 2 states or in matrices with missing data  -- see the examples in  table \ref{noPairwiseCompat} from \citet{Felsenstein2004} and \citet{Fitch1975}.
This does not mean that you cannot use maximum compatibility as a criterion for evaluating trees.
Nor does it mean that the correspondence between ML under the ``perfect + noise'' model and maximum compatibility will be disrupted. 
It merely means that we cannot (necessarily) find the maximum compatibility tree by constructing
a compatibility graph of the character patterns and finding the largest weight clique.

\begin{table}[htdp]
\begin{center}
\caption{Two data matrices for which the pairwise compatibility theorem does not apply}\label{noPairwiseCompat}
\begin{tabular}{|c|c|c|c|}
\hline 
Taxon & & &  \\ 
\hline 
A & 0 & 0 & 0 \\
B & ? & 0 & 1 \\
C & 1 & ? & 0 \\
D & 0 & 1 & ? \\
E & 1 & 1 & 1 \\
\hline 
\end{tabular}
\begin{tabular}{|c|c|c|c|}
\hline 
Taxon & & &  \\ 
\hline 
A & 0 & 0 & 0 \\
B & 0 & 1 & 2 \\
C & 1 & 1 & 1 \\
D & 1 & 2 & 0 \\
E & 2 & 2 & 2 \\
\hline 
\end{tabular}
\end{center}
\end{table}

\bibliography{phylo}
\end{document}




\section*{Is our model reasonable?}
We have just derived a model that justifies the use of selection of trees using the maximum compatibility.
Is the model biologically plausible?

No, and it is not even close.  
First of all, the perfect data portion of the model assumed that (for characters within this class) 
homoplasy is impossible -- that seems too drastic for the types of characters that we usually study.
Second the ``noise'' category assumes that the imperfect characters contain {\bf no} phylogenetic
information -- that also seems extreme. 
This could happen if the noisy characters were evolving at an extremely high (essentially infinite) rate
but this is not too plausible \citep[][provides a derivation of the connection between maximum compatibility and a mixture of low rate and high rate characters like the one presented above]{Felsenstein1981b}

We will discuss model selection later in the class. 
Very briefly, it is done be comparing the likelihood between alternate models and 
assessing whether or not there is ``significantly'' better fit base on the likelihood ratio
and some description of how complex the two models are.

We don't always need another model in hand to ask if a model is a good fit for the data.
For example one implication under our perfect+noise model is that we would see no phylogenetic
signal in the characters that do not fit the tree perfectly.
In fact if we use some summary statistic such as the minimum number of changes required to explain a 
character (the parsimony score), then we find that real data sets show much more 
phylogenetic signal among characters that have some homoplasy than we would expect if all
the homoplasy were being generated by a +noise type of model (PTP results from the Graybeal Cannatella dataset).

\section*{More realistic models that produce character conflict}
Rather than assume that incorrect homology statements are an ``all-or-nothing'' 
situation (perfect coding for a column or no information at all), it seems reasonable
to consider a model in which multiple changes can occur across the tree. 
Because the state space is not infinite this can result in homplasy -- convergence, parallelism,
reversion.  
In all cases we could imagine the homplasy to be ``biological'' (actual acquisition of the same
state in every possible detail), or the result of mis-coding similar states as the same discrete 
state code. 
While these two forms of homoplasy sound very different, they really blend into each other.
And it may not be crucial to distinguish between them when modeling morphological data.

\subsection*{No filtering of data}
Let's develop the next few sections without conditioning on the fact that our data is variable 
(the results won't change substantively for the points that we'd like to make, but it will make
the formulation more straightforward).

\section*{More realistic models that produce character conflict}
For the time being, we will continue to consider models for which there is an equal probability 
of change across each branch of the tree.
But in this case we will allow more than one change across the tree.

For the four taxon tree shown in figure \ref{ablabeledTree}, there are 5 branches, so there could be
from zero to five transitions on the tree (if we consider only the endpoints of an edge and assume
that we cannot detect multiple state changes across a single branch).
On this per-branch basis  there are $2^5=32$ possible character histories possible.
However, if we polarize the characters with 0 as the state in A, then we see recall that there are
only 8 possible patterns.
The difference in numbers is accounted for by the fact the there are two internal nodes (E and F in figure \ref{ablabeledTree}) that are unsampled. 
Each of the internal nodes can have any of the states so there are $2^2 = 4$ different ways to 
obtain each pattern.

Let's refer to the probability of a change across a single edge (any edge in the tree) as $p$:

If there are only two states, then there are only two possible outcomes for transitions across
an edge: no change, or change to the other state.
By the law of total probability, the probability of no change must be $1-p$.

By assuming that a change (or lack of change) on one branch of the tree is independent of the
probability of change on another branch, we can calculate probability of a character history as the 
product of the probabilities of the transitions.

{
\oddsidemargin = -1.0 in
\evensidemargin = 0.0 in


Table \ref{ABoneBranchLikelihood}
\begin{table}[htdp]
\begin{center}
\caption{The likelihood of data patterns on the tree shown in figure \ref{ablabeledTree}. 
The four middle columns are the likelihood of the pattern with specific states for the internal
nodes (E and F in the figure).  The last column (the pattern likelihood) is simply the sum of these four history likelihooods.}\label{ABoneBranchLikelihood}
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
             & \multicolumn{4}{c|}{Internal state (E,F)} &  \\ 
leaf pattern & (0,0) & (0,1) & (1,0) & (1,1) &  $\Pr(pattern|T_{AB})$ \\ 
\hline
0000  & $(1-p)^5 $ & $(1-p)^2 p^3 $ & $(1-p)^2 p^3 $ & $(1-p) p^4 $ & $(1-p)^5+2 (1-p)^2 p^3+(1-p) p^4 $ \\
0001  & $(1-p)^4 p $ & $(1-p)^3 p^2 $ & $(1-p) p^4 $ & $(1-p)^2 p^3 $ & $(1-p)^4 p+(1-p)^3 p^2+(1-p)^2 p^3+(1-p) p^4 $\\
0010  & $(1-p)^4 p $ & $(1-p)^3 p^2 $ & $(1-p) p^4 $ & $(1-p)^2 p^3 $ & $(1-p)^4 p+(1-p)^3 p^2+(1-p)^2 p^3+(1-p) p^4 $\\
0011  & $(1-p)^3 p^2 $ & $(1-p)^4 p $ & $p^5 $ & $(1-p)^3 p^2 $ & $(1-p)^4 p+2 (1-p)^3 p^2+p^5 $\\
0100  & $(1-p)^4 p $ & $(1-p) p^4 $ & $(1-p)^3 p^2 $ & $(1-p)^2 p^3 $ & $(1-p)^4 p+(1-p)^3 p^2+(1-p)^2 p^3+(1-p) p^4 $\\
0101  & $(1-p)^3 p^2 $ & $(1-p)^2 p^3 $ & $(1-p)^2 p^3 $ & $(1-p)^3 p^2 $ & $2 (1-p)^3 p^2+2 (1-p)^2 p^3 $\\
0110  & $(1-p)^3 p^2 $ & $(1-p)^2 p^3 $ & $(1-p)^2 p^3 $ & $(1-p)^3 p^2 $ & $2 (1-p)^3 p^2+2 (1-p)^2 p^3 $\\
0111  & $(1-p)^2 p^3 $ & $(1-p)^3 p^2 $ & $(1-p) p^4 $ & $(1-p)^4 p $ & $(1-p)^4 p+(1-p)^3 p^2+(1-p)^2 p^3+(1-p) p^4 $\\
\hline
\end{tabular}
\end{center}
\end{table}
}


\begin{figure}[htbp]
\begin{center}
\caption{Pattern frequencies as a function of the per-branch probability of change.}
\label{default}
\begin{picture}(600,200)(0,0)
	\put(0,-153){\makebox(0,0)[l]{\includegraphics[scale=1.]{../images/patFreq.pdf}}}
\end{picture}
\end{center}
\end{figure}


\newpage
\bibliography{phylo}


\end{document}